{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb9a51d",
   "metadata": {},
   "source": [
    "                                                Sub: LP-IV\n",
    "                     Name: Abhishek Bhakare                                       Roll No: 3073\n",
    "                     \n",
    "                     Name of Assignment: Implement the Continuous Bag Of Words (CBOW) Model. Task to build the model are.\n",
    "        a. Data preparation\n",
    "        b. Generate training data\n",
    "        c. Train model\n",
    "        d. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc69c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for testing purposes\n",
    "sample_text = \"\"\"\n",
    "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: \n",
    "once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, \n",
    "'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2f714",
   "metadata": {},
   "source": [
    "# Removing Special Characters, Punctuation, and Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbaa6282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nalice was beginning to get very tired of sitting by her sister on the bank and of having nothing to do \\nonce or twice she had peeped into the book her sister was reading but it had no pictures or conversations in it \\nand what is the use of a book thought alice without pictures or conversation\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Remove special characters and digits\n",
    "text = re.sub(r'[^A-Za-z\\s]', '', sample_text)\n",
    "#Convert text to lowercase\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6565d06a",
   "metadata": {},
   "source": [
    "# Tokenize the Document into Sentences and Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d181f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', 'and', 'of', 'having', 'nothing', 'to', 'do', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', 'thought', 'alice', 'without', 'pictures', 'or', 'conversation']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize into sentences (split by periods)\n",
    "sentences = text.split('.')\n",
    "\n",
    "# Tokenize each sentence into words\n",
    "tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "print(tokenized_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b654ab",
   "metadata": {},
   "source": [
    " # Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086f9676",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m----> 2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load English stop words\u001b[39;00m\n\u001b[0;32m      4\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# Load English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Remove stop words from the tokenized sentences\n",
    "filtered_sentences = [[word for word in sentence if word not in stop_words] for sentence in tokenized_sentences]\n",
    "print(filtered_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b222235",
   "metadata": {},
   "source": [
    "# Train the CBOW Model (Using Gensim Word2Vec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b13c70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train the Word2Vec model using CBOW (default is CBOW if sg=0)\n",
    "model = Word2Vec(sentences=filtered_sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('cbow_model_sample.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a18129b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'alice': [('thought', 0.21885764598846436), ('bank', 0.21617142856121063), ('peeped', 0.0931011214852333), ('tired', 0.09291722625494003), ('without', 0.07964925467967987), ('conversation', 0.0628441646695137), ('use', 0.05435018613934517), ('sitting', 0.02706393599510193), ('get', 0.01611921563744545), ('pictures', -0.010837165638804436)]\n",
      "Predicted words for context: [('use', 0.055557035), ('beginning', 0.05555604), ('pictures', 0.055555962), ('tired', 0.055555895), ('get', 0.05555586), ('conversations', 0.05555586), ('sister', 0.05555585), ('book', 0.055555843), ('alice', 0.05555575), ('conversation', 0.055555705)]\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "loaded_model = Word2Vec.load('cbow_model_sample.model')\n",
    "\n",
    "# Find the most similar words to a specific word (e.g., \"alice\")\n",
    "similar_words = loaded_model.wv.most_similar('alice', topn=10)\n",
    "print(\"Most similar words to 'alice':\", similar_words)\n",
    "\n",
    "# Predict the output word based on a list of context words\n",
    "context_words = ['sitting', 'sister']\n",
    "predicted_words = loaded_model.predict_output_word(context_words, topn=10)\n",
    "print(\"Predicted words for context:\", predicted_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
